{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Models - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice many concepts you have learned so far, from adding interactions and polynomials to your model to regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Build a linear regression model with interactions and polynomial features \n",
    "- Use feature selection to obtain the optimal subset of features in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get Started!\n",
    "\n",
    "Below we import all the necessary packages for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"ames.csv\")\n",
    "# Subset columns\n",
    "df = df[['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF',\n",
    "         '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd',\n",
    "         'GarageArea', 'Fireplaces', 'SalePrice']]\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df['SalePrice']\n",
    "X = df.drop(columns='SalePrice')\n",
    "\n",
    "# Split into train, test, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Housing Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we imported the Ames housing data and grabbed a subset of the data to use in this analysis.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Scale all the predictors using `StandardScaler`, then convert these scaled features back into DataFrame objects\n",
    "- Build a baseline `LinearRegression` model using *scaled variables* as predictors and use the $R^2$ score to evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale X_train and X_val using StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "# Ensure X_train and X_val are scaled DataFrames\n",
    "# (hint: you can set the columns using X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868344817421309"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "model = LinearRegression()\n",
    "# Create a LinearRegression model and fit it on scaled training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate a baseline r-squared score on training data\n",
    "r2_train = model.score(X_train_scaled, y_train)\n",
    "r2_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Interactions\n",
    "\n",
    "Instead of adding all possible interaction terms, let's try a custom technique. We are only going to add the interaction terms that increase the $R^2$ score as much as possible. Specifically we are going to look for the 7 interaction terms that each cause the most increase in the coefficient of determination.\n",
    "\n",
    "### Find the Best Interactions\n",
    "\n",
    "Look at all the possible combinations of variables for interactions by adding interactions one by one to the baseline model. Create a data structure that stores the pair of columns used as well as the $R^2$ score for each combination.\n",
    "\n",
    "***Hint:*** We have imported the `combinations` function from `itertools` for you ([documentation here](https://docs.python.org/3/library/itertools.html#itertools.combinations)). Try applying this to the columns of `X_train` to find all of the possible pairs.\n",
    "\n",
    "Print the 7 interactions that result in the highest $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction: ('LotArea', '1stFlrSF'), R-squared: 0.7211105666140568\n",
      "Interaction: ('LotArea', 'TotalBsmtSF'), R-squared: 0.7071649207050109\n",
      "Interaction: ('LotArea', 'GrLivArea'), R-squared: 0.6690980823779027\n",
      "Interaction: ('LotArea', 'Fireplaces'), R-squared: 0.6529699515652588\n",
      "Interaction: ('2ndFlrSF', 'TotRmsAbvGrd'), R-squared: 0.6472994890405193\n",
      "Interaction: ('OverallCond', 'TotalBsmtSF'), R-squared: 0.642901987923377\n",
      "Interaction: ('OverallQual', '2ndFlrSF'), R-squared: 0.6422324294284367\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Set up data structure\n",
    "interaction_results = []\n",
    "\n",
    "# Find combinations of columns and loop over them\n",
    "# Make copies of X_train and X_val\n",
    "\n",
    "for combo in combinations(X_train.columns, 2):\n",
    "    X_train_copy = X_train_scaled.copy()\n",
    "    X_val_copy = X_val_scaled.copy()\n",
    "    \n",
    "    # Add interaction term to data\n",
    "    X_train_copy[f\"{combo[0]}_x_{combo[1]}\"] = X_train_copy[combo[0]] * X_train_copy[combo[1]]\n",
    "    X_val_copy[f\"{combo[0]}_x_{combo[1]}\"] = X_val_copy[combo[0]] * X_val_copy[combo[1]]\n",
    "    \n",
    "    # Find r-squared score (fit on training data, evaluate on validation data)\n",
    "    model.fit(X_train_copy, y_train)\n",
    "    r2_val = model.score(X_val_copy, y_val)\n",
    "    \n",
    "    # Append to data structure\n",
    "    interaction_results.append((combo, r2_val))\n",
    "    \n",
    "    \n",
    "# Sort and subset the data structure to find the top 7\n",
    "interaction_results_sorted = sorted(interaction_results, key=lambda x: x[1], reverse=True)[:7]\n",
    "for interaction, r2 in interaction_results_sorted:\n",
    "    print(f\"Interaction: {interaction}, R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Best Interactions\n",
    "\n",
    "Write code to include the 7 most important interactions in `X_train` and `X_val` by adding 7 columns. Use the naming convention `\"col1_col2\"`, where `col1` and `col2` are the two columns in the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Loop over top 7 interactions\n",
    "for interaction, _ in interaction_results_sorted:\n",
    "    \n",
    "    # Extract column names from data structure\n",
    "    col1, col2 = interaction\n",
    "\n",
    "    # Construct new column name\n",
    "    new_col_name = f\"{col1}_{col2}\"\n",
    "\n",
    "    \n",
    "    # Add new column to X_train and X_val\n",
    "X_train_scaled[new_col_name] = X_train_scaled[col1] * X_train_scaled[col2]\n",
    "X_val_scaled[new_col_name] = X_val_scaled[col1] * X_val_scaled[col2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Polynomials\n",
    "\n",
    "Now let's repeat that process for adding polynomial terms.\n",
    "\n",
    "### Find the Best Polynomials\n",
    "\n",
    "Try polynomials of degrees 2, 3, and 4 for each variable, in a similar way you did for interactions (by looking at your baseline model and seeing how $R^2$ increases). Do understand that when going for a polynomial of degree 4 with `PolynomialFeatures`, the particular column is raised to the power of 2 and 3 as well in other terms.\n",
    "\n",
    "We only want to include \"pure\" polynomials, so make sure no interactions are included.\n",
    "\n",
    "Once again you should make a data structure that contains the values you have tested. We recommend a list of tuples of the form:\n",
    "\n",
    "`(col_name, degree, R2)`, so eg. `('OverallQual', 2, 0.781)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: GrLivArea, Degree: 3, R-squared: 0.8208554979315524\n",
      "Column: GrLivArea, Degree: 4, R-squared: 0.7804048794615333\n",
      "Column: 2ndFlrSF, Degree: 3, R-squared: 0.6759499371294796\n",
      "Column: 2ndFlrSF, Degree: 4, R-squared: 0.6750327672736952\n",
      "Column: 1stFlrSF, Degree: 2, R-squared: 0.6739854014775069\n",
      "Column: 2ndFlrSF, Degree: 2, R-squared: 0.671827545846226\n",
      "Column: OverallQual, Degree: 4, R-squared: 0.654817644533801\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Set up data structure\n",
    "polynomial_results = []\n",
    "\n",
    "# Loop over all columns\n",
    "for col in X_train.columns:\n",
    "\n",
    "    # Loop over degrees 2, 3, 4\n",
    "     for degree in [2, 3, 4]:\n",
    "\n",
    "        # Make a copy of X_train and X_val\n",
    "        X_train_copy = X_train_scaled.copy()\n",
    "        X_val_copy = X_val_scaled.copy()\n",
    "        # Instantiate PolynomialFeatures with relevant degree\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        \n",
    "        # Fit polynomial to column and transform column\n",
    "        # Hint: use the notation df[[column_name]] to get the right shape\n",
    "        # Hint: convert the result to a DataFrame\n",
    "        X_train_poly = poly.fit_transform(X_train_copy[[col]])\n",
    "        X_val_poly = poly.transform(X_val_copy[[col]])\n",
    "        poly_col_names = [f\"{col}_poly_{d}\" for d in range(1, degree+1)]\n",
    "        X_train_poly_df = pd.DataFrame(X_train_poly, columns=poly_col_names)\n",
    "        X_val_poly_df = pd.DataFrame(X_val_poly, columns=poly_col_names)\n",
    "        \n",
    "        # Add polynomial to data\n",
    "        # Hint: use pd.concat since you're combining two DataFrames\n",
    "        # Hint: drop the column before combining so it doesn't appear twice\n",
    "        X_train_copy = X_train_copy.drop(columns=[col])\n",
    "        X_val_copy = X_val_copy.drop(columns=[col])\n",
    "        X_train_copy = pd.concat([X_train_copy, X_train_poly_df], axis=1)\n",
    "        X_val_copy = pd.concat([X_val_copy, X_val_poly_df], axis=1)\n",
    "    \n",
    "        # Find r-squared score on validation\n",
    "        model.fit(X_train_copy, y_train)\n",
    "        r2_val = model.score(X_val_copy, y_val)\n",
    "    \n",
    "        # Append to data structure\n",
    "        polynomial_results.append((col, degree, r2_val))\n",
    "\n",
    "# Sort and subset the data structure to find the top 7\n",
    "polynomial_results_sorted = sorted(polynomial_results, key=lambda x: x[2], reverse=True)[:7]\n",
    "for col, degree, r2 in polynomial_results_sorted:\n",
    "    print(f\"Column: {col}, Degree: {degree}, R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Best Polynomials\n",
    "\n",
    "If there are duplicate column values in the results above, don't add multiple of them to the model, to avoid creating duplicate columns. (For example, if column `A` appeared in your list as both a 2nd and 3rd degree polynomial, adding both would result in `A` squared being added to the features twice.) Just add in the polynomial that results in the highest R-Squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "best_polynomials = {}\n",
    "\n",
    "# Filter out duplicates\n",
    "for col, degree, r2 in polynomial_results_sorted:\n",
    "    if col not in best_polynomials or best_polynomials[col][1] < degree:\n",
    "        best_polynomials[col] = (r2, degree)\n",
    "\n",
    "# Loop over remaining results\n",
    "for col, (r2, degree) in best_polynomials.items():\n",
    "    X_train_copy = X_train_scaled.copy()\n",
    "    X_val_copy = X_val_scaled.copy()\n",
    "\n",
    "    # Create polynomial terms\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_copy[[col]])\n",
    "    X_val_poly = poly.transform(X_val_copy[[col]])\n",
    "    \n",
    "    # Concat new polynomials to X_train and X_val\n",
    "    X_train_copy = pd.concat([X_train_copy, X_train_poly_df], axis=1)\n",
    "    X_val_copy = pd.concat([X_val_copy, X_val_poly_df], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your final data set and make sure that your interaction terms as well as your polynomial terms are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set columns after adding interactions and polynomials:\n",
      "Index(['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF', '1stFlrSF',\n",
      "       '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd', 'GarageArea', 'Fireplaces',\n",
      "       'OverallQual_2ndFlrSF', 'Fireplaces_poly_1', 'Fireplaces_poly_2',\n",
      "       'Fireplaces_poly_3', 'Fireplaces_poly_4'],\n",
      "      dtype='object')\n",
      "Validation set columns after adding interactions and polynomials:\n",
      "Index(['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF', '1stFlrSF',\n",
      "       '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd', 'GarageArea', 'Fireplaces',\n",
      "       'OverallQual_2ndFlrSF', 'Fireplaces_poly_1', 'Fireplaces_poly_2',\n",
      "       'Fireplaces_poly_3', 'Fireplaces_poly_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "print(\"Training set columns after adding interactions and polynomials:\")\n",
    "print(X_train_copy.columns)\n",
    "\n",
    "print(\"Validation set columns after adding interactions and polynomials:\")\n",
    "print(X_val_copy.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model R-Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the $R^2$ of the full model with your interaction and polynomial terms added. Print this value for both the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2: 0.7945\n",
      "Validation R^2: 0.6472\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data (with interactions and polynomials)\n",
    "model.fit(X_train_copy, y_train)\n",
    "\n",
    "# Calculate R^2 score on the training data\n",
    "train_r2 = model.score(X_train_copy, y_train)\n",
    "\n",
    "# Calculate R^2 score on the validation data\n",
    "val_r2 = model.score(X_val_copy, y_val)\n",
    "\n",
    "# Print the R^2 scores for both training and validation sets\n",
    "print(f\"Training R^2: {train_r2:.4f}\")\n",
    "print(f\"Validation R^2: {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we may be overfitting some now. Let's try some feature selection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "First, test out `RFE` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) with several different `n_features_to_select` values. For each value, print out the train and validation $R^2$ score and how many features remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features_to_select = 5\n",
      "Training R^2: 0.7281\n",
      "Validation R^2: 0.6817\n",
      "Features selected: 5\n",
      "\n",
      "n_features_to_select = 10\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n",
      "n_features_to_select = 15\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n",
      "n_features_to_select = 20\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n",
      "n_features_to_select = 25\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Values for n_features_to_select to test\n",
    "n_features_values = [5, 10, 15, 20, 25]\n",
    "\n",
    "# Loop through each value of n_features_to_select\n",
    "for n in n_features_values:\n",
    "    # Initialize RFE with model and the number of features to select\n",
    "    selector = RFE(model, n_features_to_select=n)\n",
    "    \n",
    "    # Fit RFE on training data\n",
    "    selector.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the selected features (this is a boolean mask)\n",
    "    selected_features = selector.support_\n",
    "    \n",
    "    # Subset X_train and X_val using the selected features\n",
    "    X_train_rfe = X_train.loc[:, selected_features]\n",
    "    X_val_rfe = X_val.loc[:, selected_features]\n",
    "    \n",
    "    # Fit the model with selected features\n",
    "    model.fit(X_train_rfe, y_train)\n",
    "    \n",
    "    # Calculate R-squared score on training data\n",
    "    train_r2 = model.score(X_train_rfe, y_train)\n",
    "    \n",
    "    # Calculate R-squared score on validation data\n",
    "    val_r2 = model.score(X_val_rfe, y_val)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"n_features_to_select = {n}\")\n",
    "    print(f\"Training R^2: {train_r2:.4f}\")\n",
    "    print(f\"Validation R^2: {val_r2:.4f}\")\n",
    "    print(f\"Features selected: {sum(selected_features)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test out `Lasso` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)) with several different `alpha` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.1\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n",
      "alpha = 1\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n",
      "alpha = 10\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n",
      "alpha = 100\n",
      "Training R^2: 0.7868\n",
      "Validation R^2: 0.6376\n",
      "Features selected: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Values for alpha to test\n",
    "alpha_values = [0.1, 1, 10, 100]\n",
    "\n",
    "# Loop through each value of alpha\n",
    "for alpha in alpha_values:\n",
    "    # Initialize Lasso with the current alpha\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    \n",
    "    # Fit the Lasso model on the training data\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the selected features (those with non-zero coefficients)\n",
    "    selected_features = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "    # Calculate R-squared score on training data\n",
    "    train_r2 = lasso.score(X_train, y_train)\n",
    "    \n",
    "    # Calculate R-squared score on validation data\n",
    "    val_r2 = lasso.score(X_val, y_val)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"alpha = {alpha}\")\n",
    "    print(f\"Training R^2: {train_r2:.4f}\")\n",
    "    print(f\"Validation R^2: {val_r2:.4f}\")\n",
    "    print(f\"Features selected: {selected_features}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results. Which features would you choose to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn conclusion, I will choose the feature selection method that provides the best balance of model performance (based on validation ð‘…2) and the number of features. \\nI can also select the model with the highest validation and the least number of features for a simpler, more interpretable model.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your written answer here\n",
    "\"\"\"\n",
    "In conclusion, I will choose the feature selection method that provides the best balance of model performance (based on validation ð‘…2) and the number of features. \n",
    "I can also select the model with the highest validation and the least number of features for a simpler, more interpretable model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Final Model on Test Data\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "At the start of this lab, we created `X_test` and `y_test`. Prepare `X_test` the same way that `X_train` and `X_val` have been prepared. This includes scaling, adding interactions, and adding polynomial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-fa47d1ea184f>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-fa47d1ea184f>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    2ndFlrSF,\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "# Loop over the top 7 interactions\n",
    "top_interactions = [\n",
    "    GrLivArea, \n",
    "    GrLivArea,\n",
    "    2ndFlrSF,\n",
    "    2ndFlrSF, \n",
    "    1stFlrSF, \n",
    "    2ndFlrSF,\n",
    "    OverallQual, \n",
    "]\n",
    "\n",
    "for col1, col2 in top_interactions:  \n",
    "    X_test_scaled[f'{col1}_{col2}'] = X_test_scaled[col1] * X_test_scaled[col2]\n",
    "\n",
    "# Loop over the top 7 polynomials\n",
    "for col_name, degree, _ in top_polynomials:  \n",
    "    poly = PolynomialFeatures(degree)\n",
    "    \n",
    "    col_data = X_test_scaled[[col_name]]\n",
    "    \n",
    "    # Transform the column into polynomial features\n",
    "    poly_features = poly.fit_transform(col_data)\n",
    "    \n",
    "    poly_col_names = [f\"{col_name}_poly_{i}\" for i in range(1, poly_features.shape[1])]\n",
    "    poly_features = poly_features[:, 1:]  \n",
    "    \n",
    "    # Create DataFrame for polynomial terms\n",
    "    poly_df = pd.DataFrame(poly_features, columns=poly_col_names)\n",
    "    \n",
    "    # Concatenate the new polynomial terms to X_test_scaled\n",
    "    X_test_scaled = pd.concat([X_test_scaled, poly_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either `RFE` or `Lasso`, fit a model on the complete train + validation set, then find R-Squared and MSE values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared: 0.8020\n",
      "Mean Squared Error: 1386804766.2206\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit Lasso on the combined train + validation data\n",
    "X_train_val = pd.concat([X_train, X_val])  # Combine train and validation sets\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso(alpha=0.1)  # You can adjust alpha as needed\n",
    "\n",
    "# Fit the Lasso model on the combined train + validation set\n",
    "lasso.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Select features from Lasso (non-zero coefficients)\n",
    "X_train_val_selected = X_train_val.loc[:, lasso.coef_ != 0]\n",
    "\n",
    "# Train the model on the selected features\n",
    "lasso.fit(X_train_val_selected, y_train_val)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_selected = X_test.loc[:, lasso.coef_ != 0]  \n",
    "y_pred = lasso.predict(X_test_selected)\n",
    "\n",
    "# Calculate R-squared and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R-Squared: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up Ideas (Optional)\n",
    "\n",
    "### Create a Lasso Path\n",
    "\n",
    "From this section, you know that when using `Lasso`, more parameters shrink to zero as your regularization parameter goes up. In scikit-learn there is a function `lasso_path()` which visualizes the shrinkage of the coefficients while $alpha$ changes. Try this out yourself!\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py\n",
    "\n",
    "### AIC and BIC for Subset Selection\n",
    "\n",
    "This notebook shows how you can use AIC and BIC purely for feature selection. Try this code out on our Ames housing data!\n",
    "\n",
    "https://xavierbourretsicotte.github.io/subset_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to apply concepts of bias-variance tradeoff using extensions to linear models and feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
